{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c5d542",
   "metadata": {},
   "source": [
    "# Web APIs & NLP - Data Collection & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ba30b-0388-4dbb-a6f6-eede859b0261",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem Statement\n",
    "Reddit has hired my team of data scientists to prototype a machine learning model that utilizes Natural Language Processing (NLP) to determine which subreddit a post originated from, only provided its content (i.e. no title, no comments, no metadata). This model will be a binary classification model for two specific subreddits using a bag-of-words approach.\n",
    "Reddit has determined 2 requirements for our team to measure success:\n",
    "1. Accuracy of classification > 95%\n",
    "2. Both Precision and Recall must remain > 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c81f49",
   "metadata": {},
   "source": [
    "## Background\n",
    "Identifying the subreddit origin of a random post on Reddit is a challenging task that requires an in-depth understanding of the language, style, and content of different subreddits. In a world where misinformation and disinformation are prevalent, being able to accurately classify posts to their original source can significantly impact the credibility and reliability of the information being shared.\n",
    "\n",
    "Reddit has hired my team of data scientists to prototype a machine learning model that utilizes NLP to determine which subreddit a post comes from only given its content (i.e. no title, no comments, no metadata). This is desirable because this concept can be expanded to other inputs and data generated by Reddit users that can be utilized in moderation, targeted marketing, and trend analysis.\n",
    "\n",
    "Because this is a proof-of-concept model that my team is building, we have confirmed with Reddit that we will build a model that will be trained for binary classification on two specific subreddits that have content related to one another. The principle for limiting the scope to this degree is to prove that the concept is functional with a small set of data before allocating significant financial resources to build a generalized model for the entire website. It is critical that the subreddit topics be related to prove that this method can function even when subreddit posts may appear similar. Provided the proof-of-concept model is successful, Reddit will then focus more resources on improving the model and generalizing it to the entire website.\n",
    "\n",
    "As mentioned in the problem statement, Reddit's metrics for success are achieving an accuracy that exceeds 95% whilst also maintaining precision and recall above 90%. The purpose of these numbers is that Reddit wants to ensure that the model is accurate and has a balanced performance.\n",
    "\n",
    "To achieve these targets, our team will use a bag-of-words approach. We will use a common API to scrape the text of posts on the chosen subreddits and then test multiple methods of pre-processing. We will test multiple different models as well as look at ensembling the best ones.\n",
    "\n",
    "This repository contains the code in the form of a scientific notebook report, the data, and the models that were used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c791f6",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "- [Problem Statement](#Problem-Statement)\n",
    "- [Background](#Background)\n",
    "- [Datasets](#Datasets)\n",
    "- [Imports](#Imports)\n",
    "- [Data Collection](#Data-Collection)\n",
    "- [Save Collected Data](#Save-Collected-Data)\n",
    "- [Data Collection Summary](#Data-Collection-Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d02e62",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "Data sets were collected from two subreddits on [Reddit.com](https://reddit.com) by scraping the data using the [Pushshift API](https://github.com/pushshift/api). The two subreddits that were selected were [*r/audiophile*](https://www.reddit.com/r/audiophile) and [*r/guitar*](https://www.reddit.com/r/guitar) because they have a sufficient amount of data as well as being related in topic.\n",
    "\n",
    "### Data Dictionary:\n",
    "The data dictionary contains all features, provided and engineered, that were used in the models.\n",
    "\n",
    "\n",
    "|Feature|Type|Dataset|Description|\n",
    "|---|---|---|---|\n",
    "|**subreddit**|*str*|Reddit|The name of the subreddit a post was scraped from|\n",
    "|**created_utc**|*str*|Reddit|The epoch timestamp of when the post was created|\n",
    "|**post_length**|*int*|Reddit|The length of each post by character|\n",
    "|**post_word_count**|*int*|Reddit|The length of each post by word count|\n",
    "|**selftext**|*str*|Reddit|The text from a post on a subreddit|\n",
    "|**cleaned_selftext**|*str*|Reddit|The cleaned text from a post on a subreddit (used for testing, but not in final model)|\n",
    "|**no_stop_selftext**|*str*|Reddit|The cleaned text from a post on a subreddit with stopwords removed (used for testing, but not in final model)|\n",
    "|**stem_selftext**|*str*|Reddit|The cleaned and stemmed text from a post on a subreddit with stopwords removed (used for testing, but not in final model)|\n",
    "|**lemmatize_selftext**|*str*|Reddit|The cleaned and lemmatized text from a post on a subreddit with stopwords removed (used for testing, but not in final model)|\n",
    "|**no_shared_stem_selftext**|*str*|Reddit|The cleaned and stemmed text from a post on a subreddit with stopwords and commonly shared words removed|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b083ad",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf07ee9",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae5651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f67131",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ea2c1",
   "metadata": {},
   "source": [
    "Data collection from Reddit is self-contained within a function in order to promote efficiency when scraping data.\n",
    "\n",
    "https://www.epochconverter.com/ was used to calculate and convert hardcoded epoch timestamps.\n",
    "\n",
    "### get_subreddit_posts\n",
    "- This function uses the pushshift API to collect data from Reddit.\n",
    "- It performs some preliminary data cleaning in order to ensure that the collected samples do not contain any blanks and are unique from one another.\n",
    "- Posts collected must conatin more than 10 words so the model has enough data to train on.\n",
    "- As it iterates, it prints out the number of samples it collected in order to monitor its progress.\n",
    "- When the function has completed running, it prints the sizes of each dataframe that was collected.\n",
    "- NOTE: If there are not sufficient entries from a subreddit, the function will crash. This is why the number of queried entries is also displayed. If that number dips far below the expected size (e.g. 500), then it is an indication to select a smaller 'n'-size.\n",
    "\n",
    "**Parameters:**\n",
    "- subreddits (list): List of subreddits (str) to query\n",
    "- size (int): The number of posts to be queried at a time (max: 500) (default: 100)\n",
    "- n (int): Desired number of entries that each subreddit df will return (default: 1000)\n",
    "- before (int): Epoch timestamp of the most recent time that posts will be pulled from (default: 1/1/23 1:00:00 GMT)\n",
    "\n",
    "**Returns:**\n",
    "- dfs (list): A list of pandas dataframes of length (n, len(subreddits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30384d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit_posts(subreddits, size=100, n=1000, before=1672534800):\n",
    "    \"\"\"This function queries subreddit posts using the Pushshift API\n",
    "    \n",
    "    Parameters:\n",
    "        subreddits (list): List of subreddits (str) to query\n",
    "        size (int): The number of posts to be queried at a time (max: 500) (default: 100)\n",
    "        n (int): Desired number of entries that each subreddit df will return (default: 1000)\n",
    "        before (int): Epoch timestamp of the most recent time that posts will be pulled from (defualt: 1/1/23 1:00:00 GMT)\n",
    "    \n",
    "    Returns:\n",
    "        dfs (list): A list of pandas dataframes of length (n, len(subreddits))\n",
    "     \"\"\"\n",
    "\n",
    "    # Define api url\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "\n",
    "    dfs = [] # initialize list of dfs\n",
    "    \n",
    "    for subreddit in subreddits:\n",
    "\n",
    "        # Define parameters\n",
    "        count = 0 # initialize df entry count\n",
    "        before_itr = before # reset updated 'before' to original timestamp\n",
    "        \n",
    "        # Loop until df is filled with desired amount of unique rows\n",
    "        while count <= n:\n",
    "            # Parameters for pushshift api\n",
    "            params = {\n",
    "                'subreddit': subreddit, # name of subreddit\n",
    "                'size': size, # target number of posts to be queried\n",
    "                'before': before_itr # grabs most recent entries before this date (epoch int format)\n",
    "            }\n",
    "\n",
    "            # Request data\n",
    "            res = requests.get(url, params)\n",
    "\n",
    "            # Convert to json and pull 'data' from dict\n",
    "            data = res.json()['data']\n",
    "\n",
    "            # Convert to df\n",
    "            df_temp = pd.DataFrame(data) #convert to df for concatenation\n",
    "            df_temp = df_temp[['subreddit',\n",
    "#                                'title',\n",
    "                               'selftext',                       \n",
    "#                                'author',\n",
    "#                                'permalink',\n",
    "                               'created_utc']] # only keep important columns\n",
    "\n",
    "            # append newly returned data to df\n",
    "            if count == 0:\n",
    "                df = df_temp # initialize df\n",
    "            else:\n",
    "                df = pd.concat([df, df_temp], axis=0) # concatenate to df\n",
    "                        \n",
    "            # Drop any rows that have the post removed\n",
    "            df = df.loc[df['selftext'] != '[removed]']\n",
    "\n",
    "            # Check for any duplicated posts\n",
    "            df = df.loc[df['selftext'].duplicated() != True]\n",
    "\n",
    "            # Remove empty posts (most are removed in duplicates)\n",
    "            df = df.loc[df['selftext'] != '']\n",
    "\n",
    "            # If subreddit name does not match input, then drop\n",
    "            df['subreddit'] = df['subreddit'].str.lower()\n",
    "            df = df.loc[df['subreddit'] == subreddit]\n",
    "\n",
    "            # Update date and count\n",
    "            before_itr = int(pd.to_datetime(df_temp['created_utc'], unit='s').min().timestamp())\n",
    "            count = df.shape[0]\n",
    "            print(f\"Number of posts collected: {count} ({len(data)} queried)\")\n",
    "\n",
    "            # Drop NaNs\n",
    "            df = df.dropna()\n",
    "\n",
    "            # Drop any rows that have less than 10 words\n",
    "            df = df.loc[df['selftext'].str.split().str.len() >= 10]\n",
    "            \n",
    "            # Drop specific column with no text - This was found later and has not actual words.\n",
    "            df = df.loc[df['selftext'] != \"\\^        \\^         \\^         \\^        \\^\\n\\n|        |         |         |        |\\n\\n|        |         |         |        |\"]\n",
    "\n",
    "        # Reset index\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        # Drop any rows above target number of rows\n",
    "        df = df.iloc[:n]\n",
    "        \n",
    "        # Drop 'created_utc' column\n",
    "        # This was used for collecting the data in a sequential manner, but is not needed for the model analysis\n",
    "        df = df.drop(columns='created_utc')\n",
    "                \n",
    "        # Create a list of dfs for different subreddits\n",
    "        dfs.append(df)\n",
    "\n",
    "        # Print collection update\n",
    "        print(f\"Data from the '{subreddit}' subreddit successfully collected.\\n\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"DataFrame shapes:\")\n",
    "    print(f\"{subreddits[0]}: {dfs[0].shape}\")\n",
    "    print(f\"{subreddits[1]}: {dfs[1].shape}\")\n",
    "    \n",
    "    # Return dfs\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b71eb84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for function\n",
    "size = 500\n",
    "num_rows = 10000\n",
    "before = 1677459600 # 2/27/23 1:00:00 AM GMT Starting time\n",
    "subreddits = ['audiophile', 'guitar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06fd611c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts collected: 273 (499 queried)\n",
      "Number of posts collected: 558 (500 queried)\n",
      "Number of posts collected: 862 (500 queried)\n",
      "Number of posts collected: 1155 (500 queried)\n",
      "Number of posts collected: 1418 (500 queried)\n",
      "Number of posts collected: 1693 (499 queried)\n",
      "Number of posts collected: 1957 (500 queried)\n",
      "Number of posts collected: 2261 (500 queried)\n",
      "Number of posts collected: 2559 (500 queried)\n",
      "Number of posts collected: 2855 (500 queried)\n",
      "Number of posts collected: 3146 (500 queried)\n",
      "Number of posts collected: 3430 (500 queried)\n",
      "Number of posts collected: 3732 (500 queried)\n",
      "Number of posts collected: 3983 (499 queried)\n",
      "Number of posts collected: 4209 (499 queried)\n",
      "Data from the 'audiophile' subreddit successfully collected.\n",
      "\n",
      "Number of posts collected: 232 (500 queried)\n",
      "Number of posts collected: 461 (498 queried)\n",
      "Number of posts collected: 700 (500 queried)\n",
      "Number of posts collected: 926 (500 queried)\n",
      "Number of posts collected: 1153 (500 queried)\n",
      "Number of posts collected: 1369 (500 queried)\n",
      "Number of posts collected: 1585 (500 queried)\n",
      "Number of posts collected: 1805 (500 queried)\n",
      "Number of posts collected: 2019 (499 queried)\n",
      "Number of posts collected: 2253 (500 queried)\n",
      "Number of posts collected: 2483 (500 queried)\n",
      "Number of posts collected: 2708 (500 queried)\n",
      "Number of posts collected: 2933 (500 queried)\n",
      "Number of posts collected: 3145 (500 queried)\n",
      "Number of posts collected: 3357 (500 queried)\n",
      "Number of posts collected: 3583 (500 queried)\n",
      "Number of posts collected: 3799 (500 queried)\n",
      "Number of posts collected: 4017 (499 queried)\n",
      "Data from the 'guitar' subreddit successfully collected.\n",
      "\n",
      "DataFrame shapes:\n",
      "audiophile: (4000, 2)\n",
      "guitar: (4000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Run function to scrape subreddits\n",
    "dfs = get_subreddit_posts(subreddits, size=500, n=4000, before = 1677459600) # before = 2/27/23 1:00:00 GMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1c37dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audiophile</td>\n",
       "      <td>Hey everyone,\\n\\nMy late father was a DJ in so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audiophile</td>\n",
       "      <td>I recently bought a Dual Kicker Comp R 12” sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audiophile</td>\n",
       "      <td>Hi I'm a beginner to this space and I just wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audiophile</td>\n",
       "      <td>I'm currently looking for a new power amp, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audiophile</td>\n",
       "      <td>i was wondering which is the best option for o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit                                           selftext\n",
       "0  audiophile  Hey everyone,\\n\\nMy late father was a DJ in so...\n",
       "1  audiophile  I recently bought a Dual Kicker Comp R 12” sub...\n",
       "2  audiophile  Hi I'm a beginner to this space and I just wan...\n",
       "3  audiophile  I'm currently looking for a new power amp, and...\n",
       "4  audiophile  i was wondering which is the best option for o..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the first df\n",
    "dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "896fd9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guitar</td>\n",
       "      <td>Hi guys, so I have my guitar connected to thre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>guitar</td>\n",
       "      <td>So I’m playing with a lot of gain but that sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>guitar</td>\n",
       "      <td>Recently bought a multiscale 8 string and play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>guitar</td>\n",
       "      <td>Will it sound uneven compared to the still-pac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>guitar</td>\n",
       "      <td>I recently put ernie ball cobalts on my guitar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                           selftext\n",
       "0    guitar  Hi guys, so I have my guitar connected to thre...\n",
       "1    guitar  So I’m playing with a lot of gain but that sho...\n",
       "2    guitar  Recently bought a multiscale 8 string and play...\n",
       "3    guitar  Will it sound uneven compared to the still-pac...\n",
       "4    guitar  I recently put ernie ball cobalts on my guitar..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the second df\n",
    "dfs[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4f9841b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Combine dfs into single final df\n",
    "df_final = pd.concat([dfs[0], dfs[1]], axis=0) # concatenate to df\n",
    "print(df_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094ea6d6",
   "metadata": {},
   "source": [
    "## Save Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67edb8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "filename = 'reddit.csv'\n",
    "df_final.to_csv(f\"../data/{filename}\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a03bb9",
   "metadata": {},
   "source": [
    "## Data Collection Summary\n",
    "- In this notebook, posts from two subreddits were collected to be processed in later notebooks.\n",
    "- The same number of posts was collected from each subreddit.\n",
    "- All posts contain text (not just the title) and are unique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
